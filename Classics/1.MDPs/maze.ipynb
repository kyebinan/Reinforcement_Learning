{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a : MDP formulation\n",
    "\n",
    "We propose the following MDP formulation: \n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "The space of state is simply the cartesian product of all the squares of the grid accessible by the player times all the squares of the grid accessible by the minotaur or the whole grid, hence :\n",
    "$$\\mathcal{s} = \\textrm{41 $\\times$ 56 states}$$\n",
    "\n",
    "#### Action Space $\\mathcal{A}$\n",
    "This are all the possible actions in the MDP\n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\textrm{up, down, right, left, stay}\\rbrace$$\n",
    "\n",
    "#### Transition Probabilities $\\mathbb{P}$\n",
    "\n",
    "- If at position $c$ taking action $a$ does not lead to a wall or an obstacle or being caught by the minotaur but to another position $c'$, then $\\mathbb{P}(c' \\vert c, a) = 1$. \n",
    "- If at  position  $c$ taking  move $a$ leads to a wall or an obstacle, the player remains in his position $c$, then $\\mathbb{P}(c \\vert c, a) = 1$.\n",
    "- For the Minotaur the transition probabilities $\\mathbb{P}(c' \\vert c, a) = 1/4$ for each move or $1/3$, $1/2$ if the minotaur is at one border or two border.\n",
    "\n",
    "Hence the Transition probabilies is $\\mathbb{P}(s' \\vert s, a) = 1/4$ for each move or $1/3$, $1/2$ if the minotaur is at one border or two border.\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles.    \n",
    "   - If at state $s$, taking action $a$, leads to a wall or an obstacle then $r(s,a) = -20$\n",
    "   - If at state $s$, taking action $a$, leads to being caught then the reward $r(s,a) = -75$\n",
    "   - If at state $s$, taking action $a$, leads to some other position in the maze that is not the exit nor a wall nor an obstacle, then $r(s, a) = -5$. \n",
    "   - If at state $s$, taking action $a$, leads to the exit then $r(s ,a) = 10$.\n",
    "\n",
    "#### Discount Factor $\\mathcal{\\Lambda}$\n",
    "The discount factor would be  $\\lambda^{(t-1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "import main \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with the convention \n",
    "# 0 = empty cell\n",
    "# 1 = obstacle\n",
    "# 2 = exit of the Maze\n",
    "\n",
    "maze = np.array ([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0 ,0, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "\n",
    "utils.draw_maze(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (0, 0), 1: (0, 1), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (1, 0), 8: (1, 1), 9: (1, 3), 10: (1, 4), 11: (1, 6), 12: (1, 7), 13: (2, 0), 14: (2, 1), 15: (2, 3), 16: (2, 4), 17: (2, 6), 18: (3, 0), 19: (3, 1), 20: (3, 2), 21: (3, 3), 22: (3, 4), 23: (3, 6), 24: (3, 7), 25: (4, 0), 26: (4, 1), 27: (4, 2), 28: (4, 3), 29: (4, 4), 30: (4, 6), 31: (4, 7), 32: (5, 0), 33: (5, 2), 34: (5, 7), 35: (6, 0), 36: (6, 1), 37: (6, 2), 38: (6, 3), 39: (6, 5), 40: (6, 6), 41: (6, 7)}\n",
      "{(0, 0): 0, (0, 1): 1, (0, 3): 2, (0, 4): 3, (0, 5): 4, (0, 6): 5, (0, 7): 6, (1, 0): 7, (1, 1): 8, (1, 3): 9, (1, 4): 10, (1, 6): 11, (1, 7): 12, (2, 0): 13, (2, 1): 14, (2, 3): 15, (2, 4): 16, (2, 6): 17, (3, 0): 18, (3, 1): 19, (3, 2): 20, (3, 3): 21, (3, 4): 22, (3, 6): 23, (3, 7): 24, (4, 0): 25, (4, 1): 26, (4, 2): 27, (4, 3): 28, (4, 4): 29, (4, 6): 30, (4, 7): 31, (5, 0): 32, (5, 2): 33, (5, 7): 34, (6, 0): 35, (6, 1): 36, (6, 2): 37, (6, 3): 38, (6, 5): 39, (6, 6): 40, (6, 7): 41}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSimpleMaze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaze\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Cantique_des_Cantiques/Project/mario/Super-Mario-RL/Classics/1.MDPs/main.py:177\u001b[0m, in \u001b[0;36mSimpleMaze.__init__\u001b[0;34m(self, maze, weights, random_rewards)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_states                 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_states_to_position)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_transitions()\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards                  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_rewards\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Cantique_des_Cantiques/Project/mario/Super-Mario-RL/Classics/1.MDPs/main.py:248\u001b[0m, in \u001b[0;36mSimpleMaze.compute_rewards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_actions):\n\u001b[1;32m    247\u001b[0m     init_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_states_to_position[s][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 248\u001b[0m     next_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     pos \u001b[38;5;241m=\u001b[39m next_pos\n\u001b[1;32m    250\u001b[0m     next_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_states_to_num[pos]\n",
      "File \u001b[0;32m~/Documents/Cantique_des_Cantiques/Project/mario/Super-Mario-RL/Classics/1.MDPs/main.py:228\u001b[0m, in \u001b[0;36mSimpleMaze.agent_move\u001b[0;34m(self, init_pos, action)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\" Makes a step in the maze, given a current position and an action.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    If the action STAY or an inadmissible action is used, the agent stays in place.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    :return tuple next_cell: Position (x,y) on the maze that agent transitions to.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Compute the future position given current (state, action)\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m y,x \u001b[38;5;241m=\u001b[39m init_pos\n\u001b[1;32m    229\u001b[0m row \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[action][\u001b[38;5;241m0\u001b[39m];\n\u001b[1;32m    230\u001b[0m col \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[action][\u001b[38;5;241m1\u001b[39m];\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "env = main.SimpleMaze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_agent     = (0,0)\n",
    "start_minotaur  = (2,4)\n",
    "horizon = 20\n",
    "\n",
    "V, policy = main.dynamic_programming(env, horizon, env.rewards, env.transition_probabilities)\n",
    "\n",
    "path, victory, policy = env.simulateDynProg(start_agent, start_minotaur, V, policy)\n",
    "print(victory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.animate_solution(env.maze, path, policy, env, 'DynProg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "env = main.Maze(maze)\n",
    "# env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_agent     = (6,3)\n",
    "start_minotaur  = (2,3)\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "\n",
    "V, policy = main.value_iteration(env, gamma, epsilon, env.rewards, env.transition_probabilities, env.n_states)\n",
    "\n",
    "path, victory, policy = env.simulateValIter(start_agent, start_minotaur, V, policy)\n",
    "print(victory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
